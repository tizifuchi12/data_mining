{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Overview\n",
        "\n",
        "This notebook explores the Munic telematics CSV exports to understand available signals, missingness patterns, and sampling properties. Findings will inform preprocessing rules for the modeling pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DATA_DIR = Path(\"../fuel_data\").resolve()\n",
        "FILES = sorted(DATA_DIR.glob(\"*.csv\"))\n",
        "print(f\"CSV files detected: {len(FILES)}\")\n",
        "FILES[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sample(file_path: Path, nrows: int = 5) -> pd.DataFrame:\n",
        "    df = pd.read_csv(file_path, nrows=nrows)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "sample_df = load_sample(FILES[0], nrows=10)\n",
        "sample_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_missingness(file_path: Path, max_rows: int = 100000) -> pd.DataFrame:\n",
        "    df = pd.read_csv(file_path, nrows=max_rows)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    summary = (\n",
        "        df.isna()\n",
        "        .mean()\n",
        "        .rename(\"missing_ratio\")\n",
        "        .to_frame()\n",
        "        .assign(non_null_count=df.notna().sum())\n",
        "    )\n",
        "    return summary\n",
        "\n",
        "missing_summary = summarize_missingness(FILES[0])\n",
        "missing_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_sampling_stats(file_path: Path, max_rows: int = 200000) -> pd.Series:\n",
        "    df = pd.read_csv(file_path, usecols=[\"time\"], nrows=max_rows)\n",
        "    df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
        "    df = df.sort_values(\"time\")\n",
        "    deltas = df[\"time\"].diff().dropna().dt.total_seconds()\n",
        "    return pd.Series(\n",
        "        {\n",
        "            \"median_dt\": deltas.median(),\n",
        "            \"mean_dt\": deltas.mean(),\n",
        "            \"std_dt\": deltas.std(),\n",
        "            \"min_dt\": deltas.min(),\n",
        "            \"max_dt\": deltas.max(),\n",
        "            \"n_samples\": len(df),\n",
        "        }\n",
        "    )\n",
        "\n",
        "sampling_stats = compute_sampling_stats(FILES[0])\n",
        "sampling_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_available_columns(file_paths, sample_size: int = 10) -> pd.Series:\n",
        "    columns_counts = {}\n",
        "    for file_path in file_paths[:sample_size]:\n",
        "        cols = pd.read_csv(file_path, nrows=1).columns.str.strip()\n",
        "        for col in cols:\n",
        "            columns_counts[col] = columns_counts.get(col, 0) + 1\n",
        "    return pd.Series(columns_counts).sort_values(ascending=False)\n",
        "\n",
        "column_presence = get_available_columns(FILES, sample_size=50)\n",
        "column_presence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning Decisions\n",
        "\n",
        "- Parse `time` and `received_at` as UTC timestamps; keep `time` as primary ordering key.\n",
        "- Use `TRACKS.MUNIC.GPS_SPEED (km/h)` as the canonical speed signal; fallback to `TRACKS.MUNIC.MDI_OBD_SPEED (km/h)` when GPS is missing but OBD is present.\n",
        "- Treat fuel metrics as cumulative (monotonic) counters; derive instantaneous consumption by differentiating after smoothing and clipping non-positive changes.\n",
        "- Remove sensor bursts with implausible values (negative speeds, fuel jumps >99th percentile) and forward-fill small gaps (â‰¤5s) before feature engineering.\n",
        "- Resample trajectories to a uniform cadence (default 1 Hz) after interpolation to support downstream modeling and frequency studies.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
